{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPAsgn2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6A_5g2aLphaU",
        "outputId": "d1f7bee9-f062-4841-f99b-9f54e5625b72"
      },
      "source": [
        "# analogies solved via glove vector addition\n",
        "import numpy as np\n",
        "from scipy import spatial\n",
        "from sklearn.manifold import TSNE\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AC81a9LTqBTo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d55368a-a442-4355-cb46-f4baa54c8a92"
      },
      "source": [
        "vectors = dict()\n",
        "\n",
        "v_file = open('/content/gdrive/My Drive/Auburn/NLP/glove.6B.50d.txt', 'r', encoding=\"utf-8\")\n",
        "\n",
        "for line in (v_file): \n",
        "    values = line.split(' ')\n",
        "    word = values[0]\n",
        "    vectors[word] = np.asarray(values[1:], 'float32')\n",
        "\n",
        "def find_similar(embedding):\n",
        "    return sorted(vectors.keys(), key=lambda word: spatial.distance.euclidean(vectors[word], embedding))\n",
        "\n",
        "print(find_similar(-vectors[\"spain\"] + vectors[\"spanish\"] + vectors[\"germany\"])[0:10])\n",
        "print(find_similar(-vectors['japan'] + vectors['tokyo'] + vectors['france'])[0:10])\n",
        "print(find_similar(-vectors['woman'] + vectors['man'] + vectors['queen'])[0:10])\n",
        "print(find_similar(-vectors['australia'] + vectors['hotdog'] + vectors['italy'])[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['german', 'germany', 'dutch', 'austrian', 'polish', 'swiss', 'french', 'berlin', 'swedish', 'russian']\n",
            "['paris', 'france', 'strasbourg', 'brussels', 'lyon', 'amsterdam', 'prohertrib', 'french', 'marseille', 'belgium']\n",
            "['queen', 'king', 'prince', 'crown', 'knight', 'coronation', 'majesty', 'lady', 'honour', 'royal']\n",
            "['crema', 'milanese', 'campari', 'gelato', 'chiquinho', 'venti', 'mostarda', 'babiÄ‡', 'malinke', 'luddite']\n"
          ]
        }
      ]
    }
  ]
}